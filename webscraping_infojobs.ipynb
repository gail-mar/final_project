{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b545561",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from urllib.parse import quote_plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b99134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "BASE_URL = \"https://www.infojobs.net/jobsearch/search-results/list.xhtml\"\n",
    "\n",
    "def fetch_jobs(keyword, pages=3):\n",
    "    jobs = []\n",
    "    query = quote_plus(keyword)\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\"\n",
    "    }\n",
    "\n",
    "    for page in range(1, pages + 1):\n",
    "        params = {\n",
    "            \"keyword\": query,\n",
    "            \"page\": page\n",
    "        }\n",
    "\n",
    "        response = requests.get(BASE_URL, headers=headers, params=params)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        cards = soup.select(\"article\")\n",
    "\n",
    "        for card in cards:\n",
    "            title = card.select_one(\"h2\")\n",
    "            company = card.select_one(\".company\")\n",
    "            location = card.select_one(\".location\")\n",
    "            link = card.find(\"a\", href=True)\n",
    "\n",
    "            jobs.append({\n",
    "                \"title\": title.get_text(strip=True) if title else None,\n",
    "                \"company\": company.get_text(strip=True) if company else None,\n",
    "                \"location\": location.get_text(strip=True) if location else None,\n",
    "                \"url\": link[\"href\"] if link else None\n",
    "            })\n",
    "\n",
    "        time.sleep(1.5)\n",
    "\n",
    "    return pd.DataFrame(jobs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37e26ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#This file contains the code neeeded to scrape a given search page from Infojobs.net.\n",
    "\n",
    "from urllib.request import Request, urlopen\n",
    "from urllib.parse import quote\n",
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "from re import search, sub\n",
    "from tqdm import tqdm\n",
    "from sys import stdout\n",
    "\n",
    "\n",
    "def scrape_search_result_page(url, driver, i):\n",
    "    \"\"\"\n",
    "    This functions takes one search page url, the selenium driver and the page number\n",
    "    and returns the list of of job offer urls it has found after scrolling on it.\n",
    "    :param url: the search page url.\n",
    "    :param driver: the selenium driver being used.\n",
    "    :param i: the number of search page (first needs special treatment).\n",
    "    :return: list with job offer urls found in the page,\n",
    "    \"\"\"\n",
    "    # Scroll the page to get the info:\n",
    "    SCROLL_PAUSE_TIME = 1\n",
    "    driver.get(url)\n",
    "    # Exception for i == 1:\n",
    "    if i == 1:\n",
    "        input('Resolve the captcha. \\n' +\n",
    "              'Select the filters you want and annotate them if you need to keep track.\\n' +\n",
    "              'Press enter when done.')\n",
    "    # We let the page load:\n",
    "    sleep(2)\n",
    "\n",
    "    page = driver.find_element_by_tag_name('body')\n",
    "\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    # Scroll down until we arrive to the bottom of the page:\n",
    "    while True:\n",
    "        page.send_keys(Keys.PAGE_DOWN)\n",
    "        sleep(SCROLL_PAUSE_TIME)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    # Convert the completely loaded html file to BS object:\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "    # Get the urls in the complete page:\n",
    "    items_panel = soup.find(\"ul\", {\"class\": \"ij-ComponentList\"})\n",
    "    offer_urls = [\"https://\" + item['href'][2:] for item in\n",
    "                  items_panel.findAll(\"a\", {\"class\": \"ij-OfferCardContent-description-title-link\"})]\n",
    "    return offer_urls\n",
    "\n",
    "\n",
    "def scrape_search_results(search_key):\n",
    "    \"\"\"\n",
    "    This funtion takes the (already formatted) search_url, obtains the total number of offers,\n",
    "    starts the selenium driver, scrapes the search results pages and return the total list of offer urls.\n",
    "    :param search_key: keywords to search for in Infojobs.\n",
    "    :return: list with all the job offer urls fetched from the results pages.\n",
    "    \"\"\"\n",
    "    base_url = 'https://www.infojobs.net/ofertas-trabajo'\n",
    "    search_url = base_url + '?keyword=' + quote(str(search_key))\n",
    "    headers = {\n",
    "        \"User-Agent\": '''Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36\n",
    "         (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.3'''\n",
    "    }\n",
    "    # Defining the driver\n",
    "    # # Defining the options\n",
    "    options = webdriver.ChromeOptions()\n",
    "    userAgent = \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.3\"\n",
    "    options.add_argument(\"start-maximized\")\n",
    "    options.add_argument(\"disable-infobars\")\n",
    "    options.add_argument(\"--incognito\")\n",
    "    options.add_argument(\"--disable-extensions\")\n",
    "    options.add_argument(f'user-agent={userAgent}')\n",
    "    # # The driver itself\n",
    "    driver = webdriver.Chrome('/usr/local/bin/chromedriver', options=options)\n",
    "    driver.implicitly_wait(10)\n",
    "    # Resolving CAPTCHA, applying filters and getting filtered url and results:\n",
    "    all_offer_urls = scrape_search_result_page(search_url, driver, 1)\n",
    "    url_with_filters = driver.current_url\n",
    "    # Getting number of results:\n",
    "    soup = BeautifulSoup(urlopen(Request(url_with_filters, headers=headers)),\n",
    "                         \"html.parser\", from_encoding=\"windows-1252\")\n",
    "    num_of_offers_text = str(soup.find(\"h1\", {\"class\": \"ij-ResultsOverview\"}).text)\n",
    "    num_of_offers_text = num_of_offers_text.replace(\",\", \"\")\n",
    "    num_results = int(search(r\"\\d*\", num_of_offers_text).group(0))\n",
    "    num_pages = int(num_results / 20) + 1\n",
    "    print()\n",
    "    print(\"Number of offers found:\", num_results)\n",
    "    print()\n",
    "    # Analyzing the rest of search pages:\n",
    "    print()\n",
    "    print(\"Analyzing search results:\")\n",
    "    print()\n",
    "    for i in tqdm(range(2, num_pages + 1), initial=1, total=num_pages, desc=\"Search pages scraped\", file=stdout):\n",
    "        page_url = sub(r\"&page=\\d\", f\"&page={i}\", url_with_filters)\n",
    "        new_urls = scrape_search_result_page(page_url, driver, i)\n",
    "        all_offer_urls = all_offer_urls + new_urls\n",
    "        sleep(1)\n",
    "    # We close the driver:\n",
    "    driver.quit()\n",
    "    return all_offer_urls"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
